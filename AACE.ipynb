{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from statistics import mean\n",
    "import csv\n",
    "from torchvision.models import efficientnet_v2_m\n",
    "from torchvision.models import EfficientNet_V2_M_Weights\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "from model.wide_res_net_cifar import WideResNet_cifar\n",
    "from model.wide_res_net_fashionmnist import WideResNet_fashionmnist\n",
    "# from model.wide_res_net_food101 import WideResNet_food101\n",
    "from model.PyramidNet import PyramidNet\n",
    "\n",
    "from model.smooth_cross_entropy import smooth_crossentropy\n",
    "\n",
    "from data_cifar100.cifar import Cifar100\n",
    "from data_cifar100_224.cifar import Cifar100_224\n",
    "from data_cifar10.cifar import Cifar10\n",
    "from data_fashionmnist.fashionmnist import fashionmnist\n",
    "from data_food101.food101 import Food101\n",
    "\n",
    "from utility.log import Log\n",
    "from utility.initialize import initialize\n",
    "from utility.step_lr import StepLR\n",
    "from utility.bypass_bn import enable_running_stats, disable_running_stats\n",
    "from adversarial_cross_entropy import AdaptiveAdversrialCrossEntropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho, adaptive, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, use_grad_norm, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        ew_norm_squared_total = 0.0  # Initialize the total squared norm of e_w\n",
    "        for group in self.param_groups:\n",
    "            if use_grad_norm:\n",
    "                scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            else:\n",
    "                scale = torch.tensor(group[\"rho\"]).to(self.param_groups[0][\"params\"][0].device)\n",
    "\n",
    "            for p in group[\"params\"]: #note that 'p' (normally) is parameters vector for a certain layer, not in dividual parameter\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                # p.add_(e_w)  # climb to the local maximum \"w + e(w)\" #SAM\n",
    "                p.sub_(e_w)  # descend to the local minimum \"w - e(w)\" #AACE\n",
    "                \n",
    "                ew_norm_squared_total += e_w.norm(p=2).pow(2)  # Accumulate the squared norm of e_w\n",
    "        \n",
    "        ew_norm = (ew_norm_squared_total ** 0.5).item()  # Calculate the total norm of e_w\n",
    "        \n",
    "        if zero_grad: self.zero_grad()\n",
    "        return grad_norm.item(), ew_norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_train(rho, iteration=0):\n",
    "    \"\"\"\n",
    "    When select experiment:\n",
    "    - WideResNet: please select one that corresdoning wiht the data, since some layer in the model need to be modified\n",
    "    - PyramidNet: curtainly work only with Cifar\n",
    "    \"\"\"\n",
    "    experiment = \"PyramidNet\" #\"WideResNet_cifar100\", \"WideResNet_cifar10\", \"WideResNet_fashinmnist\", \"WideResNet_food101\", \"PyramidNet\", \"EfficientNetV2_m\"\n",
    "    data = \"Cifar100\" #\"Cifar100\", \"Cifar100_224\", \"Cifar10\", \"fashionmnist\", \"food101\"\n",
    "    #DONT FORGET TO CHECK 'num_classes'\n",
    "\n",
    "    # define parameters\n",
    "    gpu = \"cuda:3\"\n",
    "    threads = 36\n",
    "    # rho = 0.05\n",
    "    # rho_increae = False\n",
    "    adaptive = False\n",
    "    label_smoothing = 0\n",
    "    use_grad_norm = False\n",
    "    mode = 'ada' #'ada', 'rand', 'const'\n",
    "    result_dir = \"/home/tratchatorn/SAM/sam/example/result_repeat_\" + experiment + \"_\" + data\n",
    "    file_name = f\"repeat_AACE_{mode}_GradNorm:{use_grad_norm}_rho:{rho}_{iteration}\"\n",
    "    optimizer_save_path = result_dir + \"/\" + file_name + \"_opt.pth\"\n",
    "    model_save_path = result_dir + \"/\" + file_name + \"_model.pth\"\n",
    "    csv_path = result_dir + \"/\" + file_name + \".csv\"\n",
    "\n",
    "    \n",
    "    header = [\"epoch\", \"lr\", \"avg_ce_loss\", \"avg_aace_loss\", \"avg_grad_norm\", \"avg_perturbation\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]\n",
    "    with open(csv_path, 'a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "    \n",
    "    initialize(seed=42)\n",
    "    device = torch.device(gpu)\n",
    "\n",
    "    log = Log(log_each=10)\n",
    "    \n",
    "    base_optimizer = torch.optim.SGD\n",
    "    \n",
    "    if experiment == \"WideResNet_cifar100\":\n",
    "        batch_size = 256\n",
    "        learning_rate = 0.1\n",
    "        momentum = 0.9\n",
    "        weight_decay = 0.0005\n",
    "        epochs = 200\n",
    "        depth = 28\n",
    "        width_factor = 10\n",
    "        dropout = 0\n",
    "        model = WideResNet_cifar(depth, width_factor, dropout, in_channels=3, labels=100).to(device)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, learning_rate, epochs)\n",
    "    elif experiment == \"WideResNet_cifar10\":\n",
    "        batch_size = 256\n",
    "        learning_rate = 0.1\n",
    "        momentum = 0.9\n",
    "        weight_decay = 0.0005\n",
    "        epochs = 200\n",
    "        depth = 28\n",
    "        width_factor = 10\n",
    "        dropout = 0\n",
    "        model = WideResNet_cifar(depth, width_factor, dropout, in_channels=3, labels=10).to(device)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, learning_rate, epochs)\n",
    "    elif experiment == \"PyramidNet\":\n",
    "        batch_size = 64\n",
    "        learning_rate = 0.1\n",
    "        momentum = 0.9\n",
    "        weight_decay = 0.0005\n",
    "        epochs = 200\n",
    "        depth = 272\n",
    "        alpha = 200\n",
    "        num_classes = 100\n",
    "        bottleneck = True\n",
    "        _data = \"cifar100\" #for \"food101\" use dataset=\"cifar100\"\n",
    "#         _data = \"fashionmnist\"\n",
    "        model = PyramidNet(dataset=_data, depth=depth, alpha=alpha, num_classes=num_classes).to(device)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = StepLR(optimizer, learning_rate, epochs)\n",
    "    elif experiment == \"EfficientNetV2_m\":\n",
    "        \"\"\"to overwrite dropout fix EfficientNet.py (anaconda3/envs/Jin_AACE/lib/python3.8/site-packages/torchvision/models/efficientnet.py)\n",
    "        according to https://github.com/pytorch/vision/commit/5785e2b05cdffeb39678914b8308a260e7e757db \n",
    "        or\n",
    "        update torchvision to 0.17 or newer\"\"\"\n",
    "        batch_size = 32\n",
    "        learning_rate = 0.001\n",
    "        momentum = 0.9\n",
    "        weight_decay = 0\n",
    "        epochs = 50\n",
    "        dropout = 0\n",
    "        num_classes = 100\n",
    "        pre_trained_weights = EfficientNet_V2_M_Weights.IMAGENET1K_V1 # EfficientNet_V2_M_Weights.IMAGENET1K_V1, None\n",
    "        model = efficientnet_v2_m(weights=pre_trained_weights, dropout=dropout) #use pre-trained weights\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes) #change last dense layer to be consistent with training data\n",
    "        model = model.to(device)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "#         scheduler = StepLR(optimizer, learning_rate, epochs)\n",
    "    elif experiment == \"vit_b_16\":\n",
    "        batch_size = 32\n",
    "        learning_rate = 0.001\n",
    "        momentum = 0.9\n",
    "        weight_decay = 0\n",
    "        epochs = 100\n",
    "        dropout = 0\n",
    "        num_classes = 100\n",
    "        image_size = 224\n",
    "        pre_trained_weights = ViT_B_16_Weights.IMAGENET1K_V1 # weights=ViT_B_16_Weights.IMAGENET1K_V1, None\n",
    "        model = vit_b_16(weights=pre_trained_weights, image_size=image_size)\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, num_classes) #change last dense layer to be consistent with training data\n",
    "        model = model.to(device)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "        \n",
    "        \n",
    "    if data == \"Cifar100\":\n",
    "        dataset = Cifar100(batch_size, threads)\n",
    "    elif data == \"Cifar100_224\":\n",
    "        dataset = Cifar100_224(batch_size, threads)\n",
    "    elif data == \"Cifar10\":\n",
    "        dataset = Cifar10(batch_size, threads)\n",
    "    elif data == \"fashionmnist\":\n",
    "        dataset = fashionmnist(batch_size, threads)\n",
    "    elif data == \"food101\":\n",
    "        dataset = Food101(batch_size, threads)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "#         if rho_increae == True:\n",
    "#             current_rho = (2*rho) * (epoch/epochs)\n",
    "#             optimizer = SAM(model.parameters(), base_optimizer, rho=current_rho, adaptive=adaptive, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        result_list = []\n",
    "        model.train()\n",
    "        log.train(len_dataset=len(dataset.train))\n",
    "        \n",
    "        grad_norm_list = []\n",
    "        ew_norm_list = []\n",
    "        \n",
    "        sum_ce_loss = 0\n",
    "        sum_perturb_loss = 0\n",
    "        sum_loss_num = 0\n",
    "        \n",
    "        for batch in dataset.train:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            # first forward-backward step\n",
    "            enable_running_stats(model)\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            ce_loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing) #ce_loss is not used, just calculate it for observation purpose\n",
    "            sum_ce_loss += ce_loss.sum().item()\n",
    "            \n",
    "            loss_function = AdaptiveAdversrialCrossEntropy()\n",
    "            perturb_loss = loss_function(predictions, targets)\n",
    "            perturb_loss.mean().backward()\n",
    "            sum_perturb_loss += perturb_loss.sum().item()\n",
    "    \n",
    "            sum_loss_num += perturb_loss.size(0)\n",
    "            \n",
    "            grad_norm, ew_norm = optimizer.first_step(use_grad_norm=use_grad_norm, zero_grad=True)\n",
    "            ew_norm_list.append(ew_norm)\n",
    "            grad_norm_list.append(grad_norm)\n",
    "            \n",
    "            # second forward-backward step\n",
    "            disable_running_stats(model)\n",
    "            loss = smooth_crossentropy(model(inputs), targets, smoothing=label_smoothing)\n",
    "            loss.mean().backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                correct = torch.argmax(predictions.data, 1) == targets\n",
    "                if experiment == \"EfficientNetV2_m\" or experiment == \"vit_b_16\":\n",
    "#                 if experiment == \"vit_b_16\":\n",
    "                    log(model, loss.cpu(), correct.cpu(), optimizer.param_groups[0]['lr'])\n",
    "                    scheduler.step()\n",
    "                else:\n",
    "                    log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "                    scheduler(epoch)\n",
    "        \n",
    "        avg_ce_loss = sum_ce_loss/sum_loss_num\n",
    "        avg_perturb_loss = sum_perturb_loss/sum_loss_num\n",
    "        avg_ew_norm = mean(ew_norm_list)\n",
    "        avg_grad_norm = mean(grad_norm_list)\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss, train_acc, lr = log.output_0()\n",
    "        log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataset.test:\n",
    "                inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "                predictions = model(inputs)\n",
    "                loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "                correct = torch.argmax(predictions, 1) == targets\n",
    "                log(model, loss.cpu(), correct.cpu())\n",
    "        \n",
    "        val_loss, val_acc = log.output_1()\n",
    "        log.flush()\n",
    "\n",
    "        result_list.extend([epoch, lr, avg_ce_loss, avg_perturb_loss, avg_grad_norm, avg_ew_norm, train_loss, train_acc, val_loss, val_acc])\n",
    "        with open(csv_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(result_list)\n",
    "    \n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_save_path)\n",
    "    \n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# sam_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
      "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
      "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
      "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n",
      "┃           0  ┃      3.9092  │      9.18 %  ┃   1.000e-01  │   12:35 min  ┃┈███████████████████████████┈┨      3.6182  │     13.41 %  ┃\n",
      "      3.6182  │     13.41 %  ┃\n",
      "┃           1  ┃      3.3521  │     18.33 %  ┃   1.000e-01  │   12:33 min  ┃┈███████████████████████████┈┨      3.1234  │     21.81 %  ┃\n",
      "      3.1234  │     21.81 %  ┃\n",
      "┃           2  ┃      2.9342  │     26.45 %  ┃   1.000e-01  │   12:35 min  ┃┈███████████████████████████┈┨      2.7005  │     29.97 %  ┃\n",
      "      2.7005  │     29.97 %  ┃\n",
      "┃           3  ┃      2.6345  │     33.08 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      2.3538  │     37.16 %  ┃\n",
      "      2.3538  │     37.16 %  ┃\n",
      "┃           4  ┃      2.4086  │     38.24 %  ┃   1.000e-01  │   12:34 min  ┃┈███████████████████████████┈┨      2.2830  │     39.66 %  ┃\n",
      "      2.2830  │     39.66 %  ┃\n",
      "┃           5  ┃      2.2553  │     42.20 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      2.3591  │     40.14 %  ┃\n",
      "      2.3591  │     40.14 %  ┃\n",
      "┃           6  ┃      2.1448  │     45.67 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      2.0139  │     45.76 %  ┃\n",
      "      2.0139  │     45.76 %  ┃\n",
      "┃           7  ┃      2.0525  │     48.24 %  ┃   1.000e-01  │   12:34 min  ┃┈███████████████████████████┈┨      1.9694  │     47.22 %  ┃\n",
      "      1.9694  │     47.22 %  ┃\n",
      "┃           8  ┃      1.9880  │     50.17 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.7833  │     51.71 %  ┃\n",
      "      1.7833  │     51.71 %  ┃\n",
      "┃           9  ┃      1.9256  │     52.33 %  ┃   1.000e-01  │   12:35 min  ┃┈███████████████████████████┈┨      1.8944  │     49.26 %  ┃\n",
      "      1.8944  │     49.26 %  ┃\n",
      "┃          10  ┃      1.8842  │     53.57 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.8201  │     52.12 %  ┃\n",
      "      1.8201  │     52.12 %  ┃\n",
      "┃          11  ┃      1.8395  │     54.76 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.7732  │     51.92 %  ┃\n",
      "      1.7732  │     51.92 %  ┃\n",
      "┃          12  ┃      1.8059  │     56.07 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.6167  │     55.52 %  ┃\n",
      "      1.6167  │     55.52 %  ┃\n",
      "┃          13  ┃      1.7903  │     56.53 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.6457  │     54.58 %  ┃\n",
      "      1.6457  │     54.58 %  ┃\n",
      "┃          14  ┃      1.7513  │     57.84 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.4619  │     59.31 %  ┃\n",
      "      1.4619  │     59.31 %  ┃\n",
      "┃          15  ┃      1.7350  │     58.45 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.5120  │     58.55 %  ┃\n",
      "      1.5120  │     58.55 %  ┃\n",
      "┃          16  ┃      1.7225  │     58.76 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.6036  │     56.13 %  ┃\n",
      "      1.6036  │     56.13 %  ┃\n",
      "┃          17  ┃      1.7004  │     59.40 %  ┃   1.000e-01  │   12:29 min  ┃┈███████████████████████████┈┨      1.5038  │     57.95 %  ┃\n",
      "      1.5038  │     57.95 %  ┃\n",
      "┃          18  ┃      1.6928  │     59.80 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.5958  │     56.87 %  ┃\n",
      "      1.5958  │     56.87 %  ┃\n",
      "┃          19  ┃      1.6698  │     60.58 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.5045  │     59.30 %  ┃\n",
      "      1.5045  │     59.30 %  ┃\n",
      "┃          20  ┃      1.6576  │     60.80 %  ┃   1.000e-01  │   12:29 min  ┃┈███████████████████████████┈┨      1.5072  │     59.51 %  ┃\n",
      "      1.5072  │     59.51 %  ┃\n",
      "┃          21  ┃      1.6605  │     60.67 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.5571  │     56.84 %  ┃\n",
      "      1.5571  │     56.84 %  ┃\n",
      "┃          22  ┃      1.6466  │     61.33 %  ┃   1.000e-01  │   12:33 min  ┃┈███████████████████████████┈┨      1.3516  │     62.41 %  ┃\n",
      "      1.3516  │     62.41 %  ┃\n",
      "┃          23  ┃      1.6420  │     61.69 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4294  │     60.47 %  ┃\n",
      "      1.4294  │     60.47 %  ┃\n",
      "┃          24  ┃      1.6313  │     61.69 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.5432  │     58.19 %  ┃\n",
      "      1.5432  │     58.19 %  ┃\n",
      "┃          25  ┃      1.6214  │     62.07 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4709  │     59.99 %  ┃\n",
      "      1.4709  │     59.99 %  ┃\n",
      "┃          26  ┃      1.6185  │     62.29 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4415  │     59.74 %  ┃\n",
      "      1.4415  │     59.74 %  ┃\n",
      "┃          27  ┃      1.6085  │     62.44 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3469  │     62.13 %  ┃\n",
      "      1.3469  │     62.13 %  ┃\n",
      "┃          28  ┃      1.6128  │     62.31 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4276  │     60.08 %  ┃\n",
      "      1.4276  │     60.08 %  ┃\n",
      "┃          29  ┃      1.5997  │     62.97 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.4671  │     59.80 %  ┃\n",
      "      1.4671  │     59.80 %  ┃\n",
      "┃          30  ┃      1.6036  │     62.71 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3472  │     62.66 %  ┃\n",
      "      1.3472  │     62.66 %  ┃\n",
      "┃          31  ┃      1.5900  │     63.00 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.4740  │     60.22 %  ┃\n",
      "      1.4740  │     60.22 %  ┃\n",
      "┃          32  ┃      1.5887  │     63.38 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.4348  │     60.23 %  ┃\n",
      "      1.4348  │     60.23 %  ┃\n",
      "┃          33  ┃      1.5831  │     63.34 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3445  │     62.59 %  ┃\n",
      "      1.3445  │     62.59 %  ┃\n",
      "┃          34  ┃      1.5847  │     63.53 %  ┃   1.000e-01  │   12:33 min  ┃┈███████████████████████████┈┨      1.3688  │     61.79 %  ┃\n",
      "      1.3688  │     61.79 %  ┃\n",
      "┃          35  ┃      1.5828  │     63.64 %  ┃   1.000e-01  │   12:33 min  ┃┈███████████████████████████┈┨      1.3818  │     61.92 %  ┃\n",
      "      1.3818  │     61.92 %  ┃\n",
      "┃          36  ┃      1.5739  │     63.69 %  ┃   1.000e-01  │   12:33 min  ┃┈███████████████████████████┈┨      1.2969  │     63.30 %  ┃\n",
      "      1.2969  │     63.30 %  ┃\n",
      "┃          37  ┃      1.5749  │     63.84 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.3388  │     63.11 %  ┃\n",
      "      1.3388  │     63.11 %  ┃\n",
      "┃          38  ┃      1.5681  │     64.03 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.4381  │     60.92 %  ┃\n",
      "      1.4381  │     60.92 %  ┃\n",
      "┃          39  ┃      1.5639  │     64.03 %  ┃   1.000e-01  │   12:32 min  ┃┈███████████████████████████┈┨      1.5062  │     58.71 %  ┃\n",
      "      1.5062  │     58.71 %  ┃\n",
      "┃          40  ┃      1.5576  │     64.13 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3427  │     61.75 %  ┃\n",
      "      1.3427  │     61.75 %  ┃\n",
      "┃          41  ┃      1.5583  │     64.47 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3503  │     62.04 %  ┃\n",
      "      1.3503  │     62.04 %  ┃\n",
      "┃          42  ┃      1.5602  │     64.59 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4621  │     59.94 %  ┃\n",
      "      1.4621  │     59.94 %  ┃\n",
      "┃          43  ┃      1.5515  │     64.56 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3158  │     63.29 %  ┃\n",
      "      1.3158  │     63.29 %  ┃\n",
      "┃          44  ┃      1.5476  │     64.51 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.2491  │     65.06 %  ┃\n",
      "      1.2491  │     65.06 %  ┃\n",
      "┃          45  ┃      1.5499  │     64.59 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3377  │     62.13 %  ┃\n",
      "      1.3377  │     62.13 %  ┃\n",
      "┃          46  ┃      1.5515  │     64.45 %  ┃   1.000e-01  │   12:29 min  ┃┈███████████████████████████┈┨      1.3539  │     62.22 %  ┃\n",
      "      1.3539  │     62.22 %  ┃\n",
      "┃          47  ┃      1.5434  │     64.68 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.3492  │     62.60 %  ┃\n",
      "      1.3492  │     62.60 %  ┃\n",
      "┃          48  ┃      1.5446  │     64.79 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.4523  │     60.31 %  ┃\n",
      "      1.4523  │     60.31 %  ┃\n",
      "┃          49  ┃      1.5357  │     64.99 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.2655  │     63.69 %  ┃\n",
      "      1.2655  │     63.69 %  ┃\n",
      "┃          50  ┃      1.5406  │     64.80 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3411  │     62.23 %  ┃\n",
      "      1.3411  │     62.23 %  ┃\n",
      "┃          51  ┃      1.5426  │     64.92 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3340  │     62.98 %  ┃\n",
      "      1.3340  │     62.98 %  ┃\n",
      "┃          52  ┃      1.5360  │     64.97 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3368  │     62.52 %  ┃\n",
      "      1.3368  │     62.52 %  ┃\n",
      "┃          53  ┃      1.5305  │     64.92 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.2698  │     64.13 %  ┃\n",
      "      1.2698  │     64.13 %  ┃\n",
      "┃          54  ┃      1.5370  │     64.75 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.3580  │     62.19 %  ┃\n",
      "      1.3580  │     62.19 %  ┃\n",
      "┃          55  ┃      1.5343  │     64.98 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.3089  │     62.91 %  ┃\n",
      "      1.3089  │     62.91 %  ┃\n",
      "┃          56  ┃      1.5366  │     65.12 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.2275  │     64.88 %  ┃\n",
      "      1.2275  │     64.88 %  ┃\n",
      "┃          57  ┃      1.5296  │     65.00 %  ┃   1.000e-01  │   12:31 min  ┃┈███████████████████████████┈┨      1.3180  │     62.78 %  ┃\n",
      "      1.3180  │     62.78 %  ┃\n",
      "┃          58  ┃      1.5319  │     64.91 %  ┃   1.000e-01  │   12:30 min  ┃┈███████████████████████████┈┨      1.3956  │     61.49 %  ┃\n",
      "      1.3956  │     61.49 %  ┃\n",
      "┃          59  ┃      1.5264  │     65.30 %  ┃   1.000e-01  │   12:29 min  ┃┈███████████████████████████┈┨      1.2510  │     65.04 %  ┃\n",
      "      1.2510  │     65.04 %  ┃\n",
      "┃          60  ┃      1.1571  │     76.12 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.8026  │     76.12 %  ┃\n",
      "      0.8026  │     76.12 %  ┃\n",
      "┃          61  ┃      1.1261  │     78.71 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.7737  │     76.99 %  ┃\n",
      "      0.7737  │     76.99 %  ┃\n",
      "┃          62  ┃      1.1386  │     79.95 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.7524  │     77.50 %  ┃\n",
      "      0.7524  │     77.50 %  ┃\n",
      "┃          63  ┃      1.1631  │     80.64 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.7406  │     78.01 %  ┃\n",
      "      0.7406  │     78.01 %  ┃\n",
      "┃          64  ┃      1.1722  │     80.82 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.7400  │     77.92 %  ┃\n",
      "      0.7400  │     77.92 %  ┃\n",
      "┃          65  ┃      1.1870  │     81.36 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.7260  │     78.13 %  ┃\n",
      "      0.7260  │     78.13 %  ┃\n",
      "┃          66  ┃      1.1952  │     81.74 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.7279  │     77.86 %  ┃\n",
      "      0.7279  │     77.86 %  ┃\n",
      "┃          67  ┃      1.1956  │     82.13 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.7455  │     77.93 %  ┃\n",
      "      0.7455  │     77.93 %  ┃\n",
      "┃          68  ┃      1.2026  │     82.40 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.7338  │     78.10 %  ┃\n",
      "      0.7338  │     78.10 %  ┃\n",
      "┃          69  ┃      1.2064  │     82.55 %  ┃   2.000e-02  │   12:37 min  ┃┈███████████████████████████┈┨      0.7351  │     78.04 %  ┃\n",
      "      0.7351  │     78.04 %  ┃\n",
      "┃          70  ┃      1.2025  │     82.87 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.7104  │     78.72 %  ┃\n",
      "      0.7104  │     78.72 %  ┃\n",
      "┃          71  ┃      1.2015  │     83.37 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.7255  │     78.45 %  ┃\n",
      "      0.7255  │     78.45 %  ┃\n",
      "┃          72  ┃      1.1993  │     83.43 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.7131  │     78.57 %  ┃\n",
      "      0.7131  │     78.57 %  ┃\n",
      "┃          73  ┃      1.2015  │     83.53 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.7113  │     78.41 %  ┃\n",
      "      0.7113  │     78.41 %  ┃\n",
      "┃          74  ┃      1.2022  │     83.98 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.7185  │     78.61 %  ┃\n",
      "      0.7185  │     78.61 %  ┃\n",
      "┃          75  ┃      1.1932  │     83.97 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.7147  │     78.68 %  ┃\n",
      "      0.7147  │     78.68 %  ┃\n",
      "┃          76  ┃      1.1982  │     84.24 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.7002  │     78.98 %  ┃\n",
      "      0.7002  │     78.98 %  ┃\n",
      "┃          77  ┃      1.1882  │     84.63 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6896  │     78.87 %  ┃\n",
      "      0.6896  │     78.87 %  ┃\n",
      "┃          78  ┃      1.1922  │     84.70 %  ┃   2.000e-02  │   12:27 min  ┃┈███████████████████████████┈┨      0.6935  │     79.06 %  ┃\n",
      "      0.6935  │     79.06 %  ┃\n",
      "┃          79  ┃      1.1838  │     84.80 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6941  │     79.41 %  ┃\n",
      "      0.6941  │     79.41 %  ┃\n",
      "┃          80  ┃      1.1866  │     84.93 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6950  │     79.21 %  ┃\n",
      "      0.6950  │     79.21 %  ┃\n",
      "┃          81  ┃      1.1796  │     85.29 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.6827  │     79.68 %  ┃\n",
      "      0.6827  │     79.68 %  ┃\n",
      "┃          82  ┃      1.1810  │     85.38 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6919  │     79.15 %  ┃\n",
      "      0.6919  │     79.15 %  ┃\n",
      "┃          83  ┃      1.1758  │     85.78 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6663  │     80.10 %  ┃\n",
      "      0.6663  │     80.10 %  ┃\n",
      "┃          84  ┃      1.1718  │     85.83 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6592  │     80.21 %  ┃\n",
      "      0.6592  │     80.21 %  ┃\n",
      "┃          85  ┃      1.1731  │     85.87 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6763  │     79.52 %  ┃\n",
      "      0.6763  │     79.52 %  ┃\n",
      "┃          86  ┃      1.1689  │     86.07 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6734  │     79.97 %  ┃\n",
      "      0.6734  │     79.97 %  ┃\n",
      "┃          87  ┃      1.1714  │     86.20 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6702  │     79.85 %  ┃\n",
      "      0.6702  │     79.85 %  ┃\n",
      "┃          88  ┃      1.1618  │     86.34 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.6879  │     79.32 %  ┃\n",
      "      0.6879  │     79.32 %  ┃\n",
      "┃          89  ┃      1.1623  │     86.55 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6695  │     80.01 %  ┃\n",
      "      0.6695  │     80.01 %  ┃\n",
      "┃          90  ┃      1.1636  │     86.63 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.6700  │     80.26 %  ┃\n",
      "      0.6700  │     80.26 %  ┃\n",
      "┃          91  ┃      1.1571  │     86.89 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6580  │     80.30 %  ┃\n",
      "      0.6580  │     80.30 %  ┃\n",
      "┃          92  ┃      1.1586  │     87.01 %  ┃   2.000e-02  │   12:28 min  ┃┈███████████████████████████┈┨      0.6655  │     80.03 %  ┃\n",
      "      0.6655  │     80.03 %  ┃\n",
      "┃          93  ┃      1.1541  │     87.14 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6662  │     80.20 %  ┃\n",
      "      0.6662  │     80.20 %  ┃\n",
      "┃          94  ┃      1.1536  │     87.29 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6576  │     80.45 %  ┃\n",
      "      0.6576  │     80.45 %  ┃\n",
      "┃          95  ┃      1.1530  │     87.44 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6662  │     80.14 %  ┃\n",
      "      0.6662  │     80.14 %  ┃\n",
      "┃          96  ┃      1.1455  │     87.40 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6446  │     80.30 %  ┃\n",
      "      0.6446  │     80.30 %  ┃\n",
      "┃          97  ┃      1.1488  │     87.53 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6488  │     80.46 %  ┃\n",
      "      0.6488  │     80.46 %  ┃\n",
      "┃          98  ┃      1.1508  │     87.61 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.6606  │     80.01 %  ┃\n",
      "      0.6606  │     80.01 %  ┃\n",
      "┃          99  ┃      1.1472  │     87.77 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6495  │     80.86 %  ┃\n",
      "      0.6495  │     80.86 %  ┃\n",
      "┃         100  ┃      1.1412  │     87.83 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.6461  │     80.85 %  ┃\n",
      "      0.6461  │     80.85 %  ┃\n",
      "┃         101  ┃      1.1430  │     88.03 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6455  │     80.36 %  ┃\n",
      "      0.6455  │     80.36 %  ┃\n",
      "┃         102  ┃      1.1439  │     88.11 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6491  │     80.40 %  ┃\n",
      "      0.6491  │     80.40 %  ┃\n",
      "┃         103  ┃      1.1400  │     88.08 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6447  │     80.66 %  ┃\n",
      "      0.6447  │     80.66 %  ┃\n",
      "┃         104  ┃      1.1391  │     88.29 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.6328  │     81.57 %  ┃\n",
      "      0.6328  │     81.57 %  ┃\n",
      "┃         105  ┃      1.1362  │     88.44 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6429  │     80.63 %  ┃\n",
      "      0.6429  │     80.63 %  ┃\n",
      "┃         106  ┃      1.1391  │     88.35 %  ┃   2.000e-02  │   12:33 min  ┃┈███████████████████████████┈┨      0.6504  │     80.62 %  ┃\n",
      "      0.6504  │     80.62 %  ┃\n",
      "┃         107  ┃      1.1362  │     88.48 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6385  │     81.05 %  ┃\n",
      "      0.6385  │     81.05 %  ┃\n",
      "┃         108  ┃      1.1339  │     88.74 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6593  │     80.19 %  ┃\n",
      "      0.6593  │     80.19 %  ┃\n",
      "┃         109  ┃      1.1352  │     88.71 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6444  │     80.61 %  ┃\n",
      "      0.6444  │     80.61 %  ┃\n",
      "┃         110  ┃      1.1287  │     89.02 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6436  │     80.99 %  ┃\n",
      "      0.6436  │     80.99 %  ┃\n",
      "┃         111  ┃      1.1330  │     89.09 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6299  │     81.09 %  ┃\n",
      "      0.6299  │     81.09 %  ┃\n",
      "┃         112  ┃      1.1285  │     89.03 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6531  │     80.64 %  ┃\n",
      "      0.6531  │     80.64 %  ┃\n",
      "┃         113  ┃      1.1314  │     89.25 %  ┃   2.000e-02  │   12:33 min  ┃┈███████████████████████████┈┨      0.6348  │     81.04 %  ┃\n",
      "      0.6348  │     81.04 %  ┃\n",
      "┃         114  ┃      1.1221  │     89.37 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6334  │     80.91 %  ┃\n",
      "      0.6334  │     80.91 %  ┃\n",
      "┃         115  ┃      1.1271  │     89.29 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.6358  │     81.23 %  ┃\n",
      "      0.6358  │     81.23 %  ┃\n",
      "┃         116  ┃      1.1234  │     89.38 %  ┃   2.000e-02  │   12:30 min  ┃┈███████████████████████████┈┨      0.6208  │     81.60 %  ┃\n",
      "      0.6208  │     81.60 %  ┃\n",
      "┃         117  ┃      1.1259  │     89.42 %  ┃   2.000e-02  │   12:32 min  ┃┈███████████████████████████┈┨      0.6500  │     80.75 %  ┃\n",
      "      0.6500  │     80.75 %  ┃\n",
      "┃         118  ┃      1.1245  │     89.54 %  ┃   2.000e-02  │   12:31 min  ┃┈███████████████████████████┈┨      0.6313  │     80.91 %  ┃\n",
      "      0.6313  │     80.91 %  ┃\n",
      "┃         119  ┃      1.1236  │     89.71 %  ┃   2.000e-02  │   12:29 min  ┃┈███████████████████████████┈┨      0.6398  │     80.89 %  ┃\n",
      "      0.6398  │     80.89 %  ┃\n",
      "┃         120  ┃      0.9593  │     92.34 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5521  │     83.47 %  ┃\n",
      "      0.5521  │     83.47 %  ┃\n",
      "┃         121  ┃      0.9736  │     93.15 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5503  │     83.65 %  ┃\n",
      "      0.5503  │     83.65 %  ┃\n",
      "┃         122  ┃      0.9828  │     93.34 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5469  │     83.56 %  ┃\n",
      "      0.5469  │     83.56 %  ┃\n",
      "┃         123  ┃      0.9909  │     93.42 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5464  │     83.56 %  ┃\n",
      "      0.5464  │     83.56 %  ┃\n",
      "┃         124  ┃      0.9954  │     93.70 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5426  │     83.66 %  ┃\n",
      "      0.5426  │     83.66 %  ┃\n",
      "┃         125  ┃      1.0097  │     93.70 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5446  │     83.71 %  ┃\n",
      "      0.5446  │     83.71 %  ┃\n",
      "┃         126  ┃      1.0093  │     93.84 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5417  │     83.78 %  ┃\n",
      "      0.5417  │     83.78 %  ┃\n",
      "┃         127  ┃      1.0123  │     94.00 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5456  │     83.75 %  ┃\n",
      "      0.5456  │     83.75 %  ┃\n",
      "┃         128  ┃      1.0238  │     93.94 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5405  │     83.88 %  ┃\n",
      "      0.5405  │     83.88 %  ┃\n",
      "┃         129  ┃      1.0225  │     94.18 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5361  │     83.60 %  ┃\n",
      "      0.5361  │     83.60 %  ┃\n",
      "┃         130  ┃      1.0279  │     94.20 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5413  │     83.62 %  ┃\n",
      "      0.5413  │     83.62 %  ┃\n",
      "┃         131  ┃      1.0312  │     94.22 %  ┃   4.000e-03  │   12:33 min  ┃┈███████████████████████████┈┨      0.5385  │     83.69 %  ┃\n",
      "      0.5385  │     83.69 %  ┃\n",
      "┃         132  ┃      1.0330  │     94.13 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5397  │     83.67 %  ┃\n",
      "      0.5397  │     83.67 %  ┃\n",
      "┃         133  ┃      1.0379  │     94.32 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5405  │     83.75 %  ┃\n",
      "      0.5405  │     83.75 %  ┃\n",
      "┃         134  ┃      1.0380  │     94.41 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5386  │     83.80 %  ┃\n",
      "      0.5386  │     83.80 %  ┃\n",
      "┃         135  ┃      1.0398  │     94.39 %  ┃   4.000e-03  │   12:28 min  ┃┈███████████████████████████┈┨      0.5414  │     83.70 %  ┃\n",
      "      0.5414  │     83.70 %  ┃\n",
      "┃         136  ┃      1.0461  │     94.33 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5411  │     83.88 %  ┃\n",
      "      0.5411  │     83.88 %  ┃\n",
      "┃         137  ┃      1.0528  │     94.48 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5386  │     84.15 %  ┃\n",
      "      0.5386  │     84.15 %  ┃\n",
      "┃         138  ┃      1.0502  │     94.44 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5361  │     83.95 %  ┃\n",
      "      0.5361  │     83.95 %  ┃\n",
      "┃         139  ┃      1.0522  │     94.47 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5373  │     83.86 %  ┃\n",
      "      0.5373  │     83.86 %  ┃\n",
      "┃         140  ┃      1.0516  │     94.51 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5361  │     84.00 %  ┃\n",
      "      0.5361  │     84.00 %  ┃\n",
      "┃         141  ┃      1.0518  │     94.59 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5374  │     83.73 %  ┃\n",
      "      0.5374  │     83.73 %  ┃\n",
      "┃         142  ┃      1.0578  │     94.58 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5373  │     83.86 %  ┃\n",
      "      0.5373  │     83.86 %  ┃\n",
      "┃         143  ┃      1.0617  │     94.70 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5398  │     83.80 %  ┃\n",
      "      0.5398  │     83.80 %  ┃\n",
      "┃         144  ┃      1.0645  │     94.61 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5356  │     83.93 %  ┃\n",
      "      0.5356  │     83.93 %  ┃\n",
      "┃         145  ┃      1.0618  │     94.70 %  ┃   4.000e-03  │   12:29 min  ┃┈███████████████████████████┈┨      0.5423  │     83.58 %  ┃\n",
      "      0.5423  │     83.58 %  ┃\n",
      "┃         146  ┃      1.0560  │     94.72 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5398  │     83.87 %  ┃\n",
      "      0.5398  │     83.87 %  ┃\n",
      "┃         147  ┃      1.0644  │     94.74 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5383  │     83.95 %  ┃\n",
      "      0.5383  │     83.95 %  ┃\n",
      "┃         148  ┃      1.0627  │     94.82 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5363  │     83.96 %  ┃\n",
      "      0.5363  │     83.96 %  ┃\n",
      "┃         149  ┃      1.0624  │     94.74 %  ┃   4.000e-03  │   12:32 min  ┃┈███████████████████████████┈┨      0.5403  │     83.85 %  ┃\n",
      "      0.5403  │     83.85 %  ┃\n",
      "┃         150  ┃      1.0693  │     94.81 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5311  │     83.89 %  ┃\n",
      "      0.5311  │     83.89 %  ┃\n",
      "┃         151  ┃      1.0599  │     94.87 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5402  │     83.81 %  ┃\n",
      "      0.5402  │     83.81 %  ┃\n",
      "┃         152  ┃      1.0644  │     94.85 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5393  │     83.74 %  ┃\n",
      "      0.5393  │     83.74 %  ┃\n",
      "┃         153  ┃      1.0734  │     94.77 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5296  │     84.03 %  ┃\n",
      "      0.5296  │     84.03 %  ┃\n",
      "┃         154  ┃      1.0704  │     94.95 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5301  │     84.07 %  ┃\n",
      "      0.5301  │     84.07 %  ┃\n",
      "┃         155  ┃      1.0705  │     94.98 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5324  │     84.07 %  ┃\n",
      "      0.5324  │     84.07 %  ┃\n",
      "┃         156  ┃      1.0717  │     94.85 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5329  │     83.91 %  ┃\n",
      "      0.5329  │     83.91 %  ┃\n",
      "┃         157  ┃      1.0670  │     95.00 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5336  │     84.04 %  ┃\n",
      "      0.5336  │     84.04 %  ┃\n",
      "┃         158  ┃      1.0659  │     94.96 %  ┃   4.000e-03  │   12:31 min  ┃┈███████████████████████████┈┨      0.5355  │     83.81 %  ┃\n",
      "      0.5355  │     83.81 %  ┃\n",
      "┃         159  ┃      1.0719  │     95.06 %  ┃   4.000e-03  │   12:30 min  ┃┈███████████████████████████┈┨      0.5358  │     83.94 %  ┃\n",
      "      0.5358  │     83.94 %  ┃\n",
      "┃         160  ┃      1.0220  │     95.52 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5256  │     84.19 %  ┃\n",
      "      0.5256  │     84.19 %  ┃\n",
      "┃         161  ┃      1.0132  │     95.61 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5254  │     84.18 %  ┃\n",
      "      0.5254  │     84.18 %  ┃\n",
      "┃         162  ┃      1.0163  │     95.72 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5244  │     84.22 %  ┃\n",
      "      0.5244  │     84.22 %  ┃\n",
      "┃         163  ┃      1.0256  │     95.69 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5240  │     84.37 %  ┃\n",
      "      0.5240  │     84.37 %  ┃\n",
      "┃         164  ┃      1.0302  │     95.71 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5227  │     84.38 %  ┃\n",
      "      0.5227  │     84.38 %  ┃\n",
      "┃         165  ┃      1.0320  │     95.72 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5225  │     84.22 %  ┃\n",
      "      0.5225  │     84.22 %  ┃\n",
      "┃         166  ┃      1.0295  │     95.68 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5247  │     84.11 %  ┃\n",
      "      0.5247  │     84.11 %  ┃\n",
      "┃         167  ┃      1.0249  │     95.79 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5228  │     84.30 %  ┃\n",
      "      0.5228  │     84.30 %  ┃\n",
      "┃         168  ┃      1.0388  │     95.75 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5237  │     84.30 %  ┃\n",
      "      0.5237  │     84.30 %  ┃\n",
      "┃         169  ┃      1.0377  │     95.78 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5239  │     84.23 %  ┃\n",
      "      0.5239  │     84.23 %  ┃\n",
      "┃         170  ┃      1.0358  │     95.88 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5213  │     84.44 %  ┃\n",
      "      0.5213  │     84.44 %  ┃\n",
      "┃         171  ┃      1.0315  │     95.82 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5227  │     84.32 %  ┃\n",
      "      0.5227  │     84.32 %  ┃\n",
      "┃         172  ┃      1.0364  │     95.80 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5213  │     84.44 %  ┃\n",
      "      0.5213  │     84.44 %  ┃\n",
      "┃         173  ┃      1.0299  │     95.79 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5210  │     84.34 %  ┃\n",
      "      0.5210  │     84.34 %  ┃\n",
      "┃         174  ┃      1.0400  │     95.89 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5221  │     84.28 %  ┃\n",
      "      0.5221  │     84.28 %  ┃\n",
      "┃         175  ┃      1.0445  │     95.84 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5199  │     84.52 %  ┃\n",
      "      0.5199  │     84.52 %  ┃\n",
      "┃         176  ┃      1.0357  │     95.97 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5224  │     84.29 %  ┃\n",
      "      0.5224  │     84.29 %  ┃\n",
      "┃         177  ┃      1.0364  │     95.88 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5230  │     84.46 %  ┃\n",
      "      0.5230  │     84.46 %  ┃\n",
      "┃         178  ┃      1.0394  │     95.83 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5209  │     84.51 %  ┃\n",
      "      0.5209  │     84.51 %  ┃\n",
      "┃         179  ┃      1.0368  │     95.97 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5245  │     84.27 %  ┃\n",
      "      0.5245  │     84.27 %  ┃\n",
      "┃         180  ┃      1.0481  │     95.75 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5234  │     84.49 %  ┃\n",
      "      0.5234  │     84.49 %  ┃\n",
      "┃         181  ┃      1.0448  │     95.87 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5208  │     84.27 %  ┃\n",
      "      0.5208  │     84.27 %  ┃\n",
      "┃         182  ┃      1.0374  │     96.01 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5209  │     84.32 %  ┃\n",
      "      0.5209  │     84.32 %  ┃\n",
      "┃         183  ┃      1.0423  │     96.02 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5184  │     84.44 %  ┃\n",
      "      0.5184  │     84.44 %  ┃\n",
      "┃         184  ┃      1.0450  │     96.09 %  ┃   8.000e-04  │   12:27 min  ┃┈███████████████████████████┈┨      0.5215  │     84.42 %  ┃\n",
      "      0.5215  │     84.42 %  ┃\n",
      "┃         185  ┃      1.0487  │     95.79 %  ┃   8.000e-04  │   12:31 min  ┃┈███████████████████████████┈┨      0.5214  │     84.36 %  ┃\n",
      "      0.5214  │     84.36 %  ┃\n",
      "┃         186  ┃      1.0416  │     95.97 %  ┃   8.000e-04  │   12:27 min  ┃┈███████████████████████████┈┨      0.5198  │     84.56 %  ┃\n",
      "      0.5198  │     84.56 %  ┃\n",
      "┃         187  ┃      1.0462  │     95.89 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5197  │     84.47 %  ┃\n",
      "      0.5197  │     84.47 %  ┃\n",
      "┃         188  ┃      1.0455  │     95.84 %  ┃   8.000e-04  │   12:27 min  ┃┈███████████████████████████┈┨      0.5216  │     84.38 %  ┃\n",
      "      0.5216  │     84.38 %  ┃\n",
      "┃         189  ┃      1.0486  │     95.95 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5220  │     84.34 %  ┃\n",
      "      0.5220  │     84.34 %  ┃\n",
      "┃         190  ┃      1.0519  │     95.97 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5218  │     84.42 %  ┃\n",
      "      0.5218  │     84.42 %  ┃\n",
      "┃         191  ┃      1.0515  │     95.88 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5228  │     84.40 %  ┃\n",
      "      0.5228  │     84.40 %  ┃\n",
      "┃         192  ┃      1.0437  │     96.04 %  ┃   8.000e-04  │   12:30 min  ┃┈███████████████████████████┈┨      0.5204  │     84.29 %  ┃\n",
      "      0.5204  │     84.29 %  ┃\n",
      "┃         193  ┃      1.0505  │     95.81 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5196  │     84.34 %  ┃\n",
      "      0.5196  │     84.34 %  ┃\n",
      "┃         194  ┃      1.0459  │     95.88 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5207  │     84.50 %  ┃\n",
      "      0.5207  │     84.50 %  ┃\n",
      "┃         195  ┃      1.0460  │     95.95 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5194  │     84.58 %  ┃\n",
      "      0.5194  │     84.58 %  ┃\n",
      "┃         196  ┃      1.0466  │     96.04 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5203  │     84.44 %  ┃\n",
      "      0.5203  │     84.44 %  ┃\n",
      "┃         197  ┃      1.0504  │     96.01 %  ┃   8.000e-04  │   12:27 min  ┃┈███████████████████████████┈┨      0.5201  │     84.33 %  ┃\n",
      "      0.5201  │     84.33 %  ┃\n",
      "┃         198  ┃      1.0537  │     95.89 %  ┃   8.000e-04  │   12:29 min  ┃┈███████████████████████████┈┨      0.5199  │     84.39 %  ┃\n",
      "      0.5199  │     84.39 %  ┃\n",
      "┃         199  ┃      1.0422  │     96.01 %  ┃   8.000e-04  │   12:28 min  ┃┈███████████████████████████┈┨      0.5225  │     84.30 %  ┃\n"
     ]
    }
   ],
   "source": [
    "rho_list = [0.2]\n",
    "\n",
    "for i, rho in enumerate(rho_list):\n",
    "    sam_train(rho, iteration=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "faLRXWIbP8_4",
    "LfXEZiHApvAb",
    "X_Dx3zr6oMj9",
    "AjUF7pTooZzw",
    "ZzhVF793MQVT",
    "zQ5hmscmuOlR"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
